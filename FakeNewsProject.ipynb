{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4d611c3f",
   "metadata": {},
   "source": [
    "## Fake News Detection with Tensorflow -- A Deep Learning Approach\n",
    "This notebook demonstrates how to build a fake news detection model using TensorFlow. The model will be trained on a dataset of news articles labeled as real or fake.\n",
    "\n",
    "#### 1. Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c5f940d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import re\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout, Bidirectional"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a37e1140",
   "metadata": {},
   "source": [
    "#### 2. Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "83f5ebb2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8476</td>\n",
       "      <td>You Can Smell Hillary’s Fear</td>\n",
       "      <td>Daniel Greenfield, a Shillman Journalism Fello...</td>\n",
       "      <td>FAKE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10294</td>\n",
       "      <td>Watch The Exact Moment Paul Ryan Committed Pol...</td>\n",
       "      <td>Google Pinterest Digg Linkedin Reddit Stumbleu...</td>\n",
       "      <td>FAKE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3608</td>\n",
       "      <td>Kerry to go to Paris in gesture of sympathy</td>\n",
       "      <td>U.S. Secretary of State John F. Kerry said Mon...</td>\n",
       "      <td>REAL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10142</td>\n",
       "      <td>Bernie supporters on Twitter erupt in anger ag...</td>\n",
       "      <td>— Kaydee King (@KaydeeKing) November 9, 2016 T...</td>\n",
       "      <td>FAKE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>875</td>\n",
       "      <td>The Battle of New York: Why This Primary Matters</td>\n",
       "      <td>It's primary day in New York and front-runners...</td>\n",
       "      <td>REAL</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                                              title  \\\n",
       "0        8476                       You Can Smell Hillary’s Fear   \n",
       "1       10294  Watch The Exact Moment Paul Ryan Committed Pol...   \n",
       "2        3608        Kerry to go to Paris in gesture of sympathy   \n",
       "3       10142  Bernie supporters on Twitter erupt in anger ag...   \n",
       "4         875   The Battle of New York: Why This Primary Matters   \n",
       "\n",
       "                                                text label  \n",
       "0  Daniel Greenfield, a Shillman Journalism Fello...  FAKE  \n",
       "1  Google Pinterest Digg Linkedin Reddit Stumbleu...  FAKE  \n",
       "2  U.S. Secretary of State John F. Kerry said Mon...  REAL  \n",
       "3  — Kaydee King (@KaydeeKing) November 9, 2016 T...  FAKE  \n",
       "4  It's primary day in New York and front-runners...  REAL  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv(\"news.csv\")\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3275cc6",
   "metadata": {},
   "source": [
    "#### 3. Data Preprocessing\n",
    "\n",
    "- Keep Necessary Columns: We'll use the title, text, and label columns. \n",
    "\n",
    "- Combine Title and Text: We will merge the title and text into a single input feature for the model. We'll also handle any missing values in the process.\n",
    "\n",
    "- Encode Labels: The model needs numerical labels, so we'll convert \"FAKE\" to 0 and \"REAL\" to 1.\n",
    "\n",
    "- Clean Text: We'll create a function to convert text to lowercase and remove punctuation, numbers, and extra spaces. This standardizes the text for the tokenizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b46a0f8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoded labels mapping:\n",
      "{'FAKE': 0, 'REAL': 1}\n",
      "\n",
      "==================================================\n",
      "\n",
      "Sample of cleaned text:\n",
      "you can smell hillarys fear daniel greenfield a shillman journalism fellow at the freedom center is a new york writer focusing on radical islam in the final stretch of the election hillary rodham clinton has gone to war with the fbi the word unprecedented has been thrown around so often this election that it ought to be retired but its still unprecedented for the nominee of a major political party to go war with the fbi but thats exactly what hillary and her people have done coma patients just waking up now and watching an hour of cnn from their hospital beds would assume that fbi director james comey is hillarys opponent in this election the fbi is under attack by everyone from obama to cnn hillarys people have circulated a letter attacking comey there are currently more media hit pieces lambasting him than targeting trump it wouldnt be too surprising if the clintons or their allies were to start running attack ads against the fbi the fbis leadership is being warned that the entire leftwing establishment will form a lynch mob if they continue going after hillary and the fbis credibility is being attacked by the media and the democrats to preemptively head off the results of the investigation of the clinton foundation and hillary clinton the covert struggle between fbi agents and obamas doj people has gone explosively public the new york times has compared comey to j edgar hoover its bizarre headline james comey role recalls hoovers fbi fairly or not practically admits up front that its spouting nonsense the boston globe has published a column calling for comeys resignation not to be outdone time has an editorial claiming that the scandal is really an attack on all women james carville appeared on msnbc to remind everyone that he was still alive and insane he accused comey of coordinating with house republicans and the kgb and you thought the vast right wing conspiracy was a stretch countless media stories charge comey with violating procedure do you know whats a procedural violation emailing classified information stored on your bathroom server senator harry reid has sent comey a letter accusing him of violating the hatch act the hatch act is a nice idea that has as much relevance in the age of obama as the tenth amendment but the cable news spectrum quickly filled with media hacks glancing at the wikipedia article on the hatch act under the table while accusing the fbi director of one of the most awkward conspiracies against hillary ever if james comey is really out to hurt hillary he picked one hell of a strange way to do it not too long ago democrats were breathing a sigh of relief when he gave hillary clinton a pass in a prominent public statement if he really were out to elect trump by keeping the email scandal going why did he trash the investigation was he on the payroll of house republicans and the kgb back then and playing it coy or was it a sudden development where vladimir putin and paul ryan talked him into taking a look at anthony weiners computer either comey is the most cunning fbi director that ever lived or hes just awkwardly trying to navigate a political mess that has trapped him between a doj leadership whose political futures are tied to hillarys victory and his own bureau whose apolitical agents just want to be allowed to do their jobs the only truly mysterious thing is why hillary and her associates decided to go to war with a respected federal agency most americans like the fbi while hillary clinton enjoys a unfavorable rating and its an interesting question hillarys old strategy was to lie and deny that the fbi even had a criminal investigation underway instead her associates insisted that it was a security review the fbi corrected her and she shrugged it off but the old breezy denial approach has given way to a savage assault on the fbi pretending that nothing was wrong was a bad strategy but it was a better one that picking a fight with the fbi while lunatic clinton associates try to claim that the fbi is really the kgb there are two possible explanations hillary clinton might be arrogant enough to lash out at the fbi now that she believes that victory is near the same kind of hubris that led her to plan her victory fireworks display could lead her to declare a war on the fbi for irritating her during the final miles of her campaign but the other explanation is that her people panicked going to war with the fbi is not the behavior of a smart and focused presidential campaign its an act of desperation when a presidential candidate decides that her only option is to try and destroy the credibility of the fbi thats not hubris its fear of what the fbi might be about to reveal about her during the original fbi investigation hillary clinton was confident that she could ride it out and she had good reason for believing that but that hillary clinton is gone in her place is a paranoid wreck within a short space of time the positive clinton campaign promising to unite the country has been replaced by a desperate and flailing operation that has focused all its energy on fighting the fbi theres only one reason for such bizarre behavior the clinton campaign has decided that an fbi investigation of the latest batch of emails poses a threat to its survival and so its gone all in on fighting the fbi its an unprecedented step born of fear its hard to know whether that fear is justified but the existence of that fear already tells us a whole lot clinton loyalists rigged the old investigation they knew the outcome ahead of time as well as they knew the debate questions now suddenly they are no longer in control and they are afraid you can smell the fear the fbi has wiretaps from the investigation of the clinton foundation its finding new emails all the time and clintonworld panicked the spinmeisters of clintonworld have claimed that the email scandal is just so much smoke without fire all thats here is the appearance of impropriety without any of the substance but this isnt how you react to smoke its how you respond to a fire the misguided assault on the fbi tells us that hillary clinton and her allies are afraid of a revelation bigger than the fundamental illegality of her email setup the email setup was a preemptive cover up the clinton campaign has panicked badly out of the belief right or wrong that whatever crime the illegal setup was meant to cover up is at risk of being exposed the clintons have weathered countless scandals over the years whatever they are protecting this time around is bigger than the usual corruption bribery sexual assaults and abuses of power that have followed them around throughout the years this is bigger and more damaging than any of the allegations that have already come out and they dont want fbi investigators anywhere near it the campaign against comey is pure intimidation its also a warning any senior fbi people who value their careers are being warned to stay away the democrats are closing ranks around their nominee against the fbi its an ugly and unprecedented scene it may also be their last stand hillary clinton has awkwardly wound her way through numerous scandals in just this election cycle but shes never shown fear or desperation before now that has changed whatever she is afraid of it lies buried in her emails with huma abedin and it can bring her down like nothing else has\n"
     ]
    }
   ],
   "source": [
    "# Keep only the 'title', 'text', and 'label' columns\n",
    "data = data[['title', 'text', 'label']]\n",
    "\n",
    "# Handle missing values in title and text\n",
    "data['title'] = data['title'].fillna('')\n",
    "data['text'] = data['text'].fillna('')\n",
    "\n",
    "# Combine title and text into a single column\n",
    "data['text'] = data['title'] + ' ' + data['text']\n",
    "data.drop(columns=['title'], inplace=True) # Drop the original title column\n",
    "\n",
    "# Drop rows where the combined text is empty\n",
    "data.dropna(subset=['text'], inplace=True)\n",
    "\n",
    "# Encode the labels\n",
    "label_encoder = LabelEncoder()\n",
    "data['label'] = label_encoder.fit_transform(data['label'])\n",
    "# FAKE -> 0, REAL -> 1\n",
    "print(\"Encoded labels mapping:\")\n",
    "print(dict(zip(label_encoder.classes_, label_encoder.transform(label_encoder.classes_))))\n",
    "print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
    "\n",
    "\n",
    "# Text cleaning function\n",
    "def clean_text(text):\n",
    "    text = text.lower()  # Lowercase text\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)  # Remove punctuation\n",
    "    text = re.sub(r'\\d+', '', text)  # Remove numbers\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()  # Remove extra whitespace\n",
    "    return text\n",
    "\n",
    "# Apply the cleaning function to the text column\n",
    "data['text'] = data['text'].apply(clean_text)\n",
    "\n",
    "print(\"Sample of cleaned text:\")\n",
    "print(data['text'].head().iloc[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "229b7585",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distribution of labels:\n",
      "label\n",
      "1    3171\n",
      "0    3164\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(\"Distribution of labels:\")\n",
    "print(data['label'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd3f8bd7",
   "metadata": {},
   "source": [
    "#### 4. Tokenization and Padding\n",
    "\n",
    "Computers don't understand words, they understand numbers. We need to convert our cleaned text into sequences of numbers.\n",
    "\n",
    "- Tokenizer: We'll use `tf.keras.preprocessing.text.Tokenizer` to build a vocabulary of the most common words and convert each article into a sequence of integers.\n",
    "\n",
    "- Padding: Neural networks require inputs of a fixed length. We'll use `pad_sequences` to make sure every sequence has the same length by adding zeros to the shorter ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "77bcec33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define parameters for tokenization and padding\n",
    "VOCAB_SIZE = 10000  # Number of words to keep in the vocabulary\n",
    "MAX_LEN = 256      # Max length of sequences\n",
    "OOV_TOKEN = \"<OOV>\" # Token for words not in the vocabulary\n",
    "\n",
    "# Initialize the tokenizer\n",
    "tokenizer = Tokenizer(num_words=VOCAB_SIZE, oov_token=OOV_TOKEN)\n",
    "tokenizer.fit_on_texts(data['text'])\n",
    "\n",
    "# Convert text to sequences of integers\n",
    "sequences = tokenizer.texts_to_sequences(data['text'])\n",
    "\n",
    "# Pad the sequences\n",
    "padded_sequences = pad_sequences(sequences, maxlen=MAX_LEN, padding='post', truncating='post')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a430992",
   "metadata": {},
   "source": [
    "#### 5. Split Data into Training and Testing Sets\n",
    "\n",
    "We need to split our data into a training set (for teaching the model) and a testing set (for evaluating its performance on unseen data). We'll use an 80/20 split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4e99778e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set shape: (5068, 256)\n",
      "Testing set shape: (1267, 256)\n"
     ]
    }
   ],
   "source": [
    "# Get features (padded sequences) and labels\n",
    "X = padded_sequences\n",
    "y = data['label'].values\n",
    "\n",
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "print(f\"Training set shape: {X_train.shape}\")\n",
    "print(f\"Testing set shape: {X_test.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71d7b0a2",
   "metadata": {},
   "source": [
    "#### 6. Build the LSTM Model\n",
    "\n",
    "- Now we define the architecture of our neural network.\n",
    "\n",
    "- Embedding Layer: This layer learns a dense vector representation for each word in our vocabulary.\n",
    "\n",
    "- Bidirectional LSTM: An LSTM (Long Short-Term Memory) layer is excellent for sequence data. We make it Bidirectional so it can learn from the text in both forward and backward directions, which improves context understanding.\n",
    "\n",
    "- Dropout: A regularization technique to prevent the model from overfitting.\n",
    "\n",
    "- Dense Layers: Standard fully connected layers for classification. The final layer uses a sigmoid activation function to output a probability between 0 and 1.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7c44689e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-26 22:29:30.146780: I metal_plugin/src/device/metal_device.cc:1154] Metal device set to: Apple M1\n",
      "2025-06-26 22:29:30.147289: I metal_plugin/src/device/metal_device.cc:296] systemMemory: 16.00 GB\n",
      "2025-06-26 22:29:30.147325: I metal_plugin/src/device/metal_device.cc:313] maxCacheSize: 5.33 GB\n",
      "2025-06-26 22:29:30.147969: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:305] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2025-06-26 22:29:30.148000: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:271] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ embedding (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)           │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ bidirectional (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Bidirectional</span>)   │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)               │ ?                      │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ bidirectional_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Bidirectional</span>) │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ ?                      │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ embedding (\u001b[38;5;33mEmbedding\u001b[0m)           │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ bidirectional (\u001b[38;5;33mBidirectional\u001b[0m)   │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout (\u001b[38;5;33mDropout\u001b[0m)               │ ?                      │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ bidirectional_1 (\u001b[38;5;33mBidirectional\u001b[0m) │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_1 (\u001b[38;5;33mDropout\u001b[0m)             │ ?                      │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                 │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "EMBEDDING_DIM = 64 # Dimension for the word vectors\n",
    "\n",
    "model = Sequential([\n",
    "    Embedding(input_dim=VOCAB_SIZE, output_dim=EMBEDDING_DIM),\n",
    "    Bidirectional(LSTM(64, return_sequences=True)),\n",
    "    Dropout(0.5),\n",
    "    Bidirectional(LSTM(32)),\n",
    "    Dropout(0.5),\n",
    "    Dense(64, activation='relu'),\n",
    "    Dense(1, activation='sigmoid') # Sigmoid for binary classification\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Print the model summary\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62fcf903",
   "metadata": {},
   "source": [
    "#### 7. Train the Model\n",
    "\n",
    "We now feed the training data to our model. We'll train for 5 epochs and use the test set as validation data to monitor performance after each epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7e3f35c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "EPOCHS = 5\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "history = model.fit(X_train, y_train,\n",
    "                    epochs=EPOCHS,\n",
    "                    batch_size=BATCH_SIZE,\n",
    "                    validation_data=(X_test, y_test),\n",
    "                    verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1c47342",
   "metadata": {},
   "source": [
    "#### 8. Evaluate the Model\n",
    "\n",
    "After training, let's see how well our model performs on the unseen test data. We expect the accuracy to be high.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "462868bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model on the test set\n",
    "loss, accuracy = model.evaluate(X_test, y_test)\n",
    "print(f\"\\nTest Accuracy: {accuracy*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02eca00c",
   "metadata": {},
   "source": [
    "#### 9. Make Predictions on New Data\n",
    "Let's use our trained model to classify new, unseen news headlines."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57f6026f",
   "metadata": {},
   "source": [
    "# Example news texts for prediction\n",
    "new_texts = [\n",
    "    \"The president announced a new groundbreaking healthcare reform today that will cover millions.\", # Likely REAL\n",
    "    \"Scientists discover aliens have been living in the ocean for centuries, controlling our weather.\", # Likely FAKE\n",
    "    \"Stock market hits an all-time high after positive economic reports are released by the government.\", # Likely REAL\n",
    "    \"You won't believe what this celebrity was caught doing, new secret photos leaked online by anonymous source.\", # Likely FAKE\n",
    "]\n",
    "\n",
    "# Preprocess the new texts\n",
    "cleaned_texts = [clean_text(text) for text in new_texts]\n",
    "sequences = tokenizer.texts_to_sequences(cleaned_texts)\n",
    "padded = pad_sequences(sequences, maxlen=MAX_LEN, padding='post', truncating='post')\n",
    "\n",
    "# Make predictions\n",
    "predictions = model.predict(padded)\n",
    "\n",
    "# Print the results\n",
    "for text, pred in zip(new_texts, predictions):\n",
    "    label = \"REAL\" if pred > 0.5 else \"FAKE\"\n",
    "    print(f\"\\nText: {text}\")\n",
    "    print(f\"Prediction: {label} (Confidence: {pred[0]:.4f})\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fakenews_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
